{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d4dcb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from collections import Counter\n",
    "import unicodedata\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c614c89b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exploratory Data Analysis\n",
    "df = pd.read_csv('IMDB Dataset.csv')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "930545cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     50000 non-null  object\n",
      " 1   sentiment  50000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 781.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec7b30e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['sentiment'].value_counts().plot(kind='barh')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8a96e87a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>am</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>be</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exploring</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>focussed</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hope</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>implement</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nlp</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stupid</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0  1  2\n",
       "am         1  0  0\n",
       "be         0  0  1\n",
       "exploring  1  0  0\n",
       "first      1  0  0\n",
       "focussed   0  0  1\n",
       "hope       0  1  0\n",
       "implement  0  1  0\n",
       "is         1  0  1\n",
       "nlp        1  0  0\n",
       "not        0  0  1\n",
       "stupid     0  0  1\n",
       "tfidf      0  1  0\n",
       "the        1  0  0\n",
       "this       1  0  0\n",
       "time       1  0  0\n",
       "to         0  1  0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's see how CountVectorizer works with an example\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text = ['This is the first time I am exploring NLP','I hope to implement TFIDF',\n",
    "       'Be Focussed is not stupid']\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "count_matrix = count_vect.fit_transform(text)\n",
    "count_array = count_matrix.toarray()\n",
    "df_1 = pd.DataFrame(data = count_array,columns = count_vect.get_feature_names_out())\n",
    "df_1.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "624cab48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>exploring</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>focussed</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hope</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>implement</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nlp</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stupid</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0  1  2\n",
       "exploring  1  0  0\n",
       "focussed   0  0  1\n",
       "hope       0  1  0\n",
       "implement  0  1  0\n",
       "nlp        1  0  0\n",
       "stupid     0  0  1\n",
       "tfidf      0  1  0\n",
       "time       1  0  0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's see how CountVectorizer works with an example, also use stop words\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text = ['This is the first time I am exploring NLP','I hope to implement TFIDF',\n",
    "       'Be Focussed is not stupid']\n",
    "\n",
    "count_vect = CountVectorizer(stop_words = 'english')\n",
    "count_matrix = count_vect.fit_transform(text)\n",
    "count_array = count_matrix.toarray()\n",
    "df_1 = pd.DataFrame(data = count_array,columns = count_vect.get_feature_names_out())\n",
    "df_1.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3b69a222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0  1  2\n",
      "exploring  1  0  0\n",
      "focussed   0  0  1\n",
      "hope       0  1  0\n",
      "implement  0  1  0\n",
      "nlp        1  0  0\n",
      "stupid     0  0  1\n",
      "tfidf      0  1  0\n",
      "time       1  0  0\n",
      "Using bigrams instead of unigrams for analysing text data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>exploring nlp</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>focussed stupid</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hope implement</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>implement tfidf</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time exploring</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0  1  2\n",
       "exploring nlp    1  0  0\n",
       "focussed stupid  0  0  1\n",
       "hope implement   0  1  0\n",
       "implement tfidf  0  1  0\n",
       "time exploring   1  0  0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Understanding ngram_range in CountVectorizer with an example\n",
    "\n",
    "text = ['This is the first time, I. am. exploring NLP','I hope to implement TFIDF',\n",
    "       'Be Focussed is not stupid']\n",
    "count_vect = CountVectorizer(stop_words = 'english',ngram_range = (1,1))\n",
    "count_matrix = count_vect.fit_transform(text)\n",
    "count_array = count_matrix.toarray()\n",
    "df_1 = pd.DataFrame(data = count_array,columns = count_vect.get_feature_names_out())\n",
    "print(df_1.T)\n",
    "\n",
    "#Using bigrams instead of unigrams for analysing text data\n",
    "text = ['This is the first time,I. am. exploring NLP','I hope to implement TFIDF',\n",
    "       'Be Focussed is not stupid']\n",
    "count_vect = CountVectorizer(stop_words = 'english',ngram_range = (2,2))\n",
    "count_matrix = count_vect.fit_transform(text)\n",
    "count_array = count_matrix.toarray()\n",
    "df_1 = pd.DataFrame(data = count_array,columns = count_vect.get_feature_names_out())\n",
    "print('Using bigrams instead of unigrams for analysing text data')\n",
    "df_1.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b3141284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'the',\n",
       " 'first1',\n",
       " 'time2',\n",
       " 'I',\n",
       " 'am',\n",
       " 'exploring',\n",
       " 'NLP',\n",
       " '79',\n",
       " '89']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Regular expression states that we can have words that start with a-zA-Z0-9\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "s = \"This is the first1 time2 I \\ $ . * am exploring NLP 79 89.\"\n",
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "token.tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3a771b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0  00  000  00000000000  0000000000001  00000001  00001  00015  000dm  \\\n",
      "0      0   0    0            0              0         0      0      0      0   \n",
      "1      0   0    0            0              0         0      0      0      0   \n",
      "2      0   0    0            0              0         0      0      0      0   \n",
      "3      0   0    0            0              0         0      0      0      0   \n",
      "4      0   0    0            0              0         0      0      0      0   \n",
      "...   ..  ..  ...          ...            ...       ...    ...    ...    ...   \n",
      "49995  0   0    0            0              0         0      0      0      0   \n",
      "49996  0   0    0            0              0         0      0      0      0   \n",
      "49997  0   0    0            0              0         0      0      0      0   \n",
      "49998  0   0    0            0              0         0      0      0      0   \n",
      "49999  0   0    0            0              0         0      0      0      0   \n",
      "\n",
      "       000s  ...  zzzzip  zzzzz  zzzzzzzz  zzzzzzzzz  zzzzzzzzzzzz  \\\n",
      "0         0  ...       0      0         0          0             0   \n",
      "1         0  ...       0      0         0          0             0   \n",
      "2         0  ...       0      0         0          0             0   \n",
      "3         0  ...       0      0         0          0             0   \n",
      "4         0  ...       0      0         0          0             0   \n",
      "...     ...  ...     ...    ...       ...        ...           ...   \n",
      "49995     0  ...       0      0         0          0             0   \n",
      "49996     0  ...       0      0         0          0             0   \n",
      "49997     0  ...       0      0         0          0             0   \n",
      "49998     0  ...       0      0         0          0             0   \n",
      "49999     0  ...       0      0         0          0             0   \n",
      "\n",
      "       zzzzzzzzzzzzz  zzzzzzzzzzzzzzzzzz  zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz  \\\n",
      "0                  0                   0                                0   \n",
      "1                  0                   0                                0   \n",
      "2                  0                   0                                0   \n",
      "3                  0                   0                                0   \n",
      "4                  0                   0                                0   \n",
      "...              ...                 ...                              ...   \n",
      "49995              0                   0                                0   \n",
      "49996              0                   0                                0   \n",
      "49997              0                   0                                0   \n",
      "49998              0                   0                                0   \n",
      "49999              0                   0                                0   \n",
      "\n",
      "       zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz  \\\n",
      "0                                              0   \n",
      "1                                              0   \n",
      "2                                              0   \n",
      "3                                              0   \n",
      "4                                              0   \n",
      "...                                          ...   \n",
      "49995                                          0   \n",
      "49996                                          0   \n",
      "49997                                          0   \n",
      "49998                                          0   \n",
      "49999                                          0   \n",
      "\n",
      "       zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz  \n",
      "0                                                 0  \n",
      "1                                                 0  \n",
      "2                                                 0  \n",
      "3                                                 0  \n",
      "4                                                 0  \n",
      "...                                             ...  \n",
      "49995                                             0  \n",
      "49996                                             0  \n",
      "49997                                             0  \n",
      "49998                                             0  \n",
      "49999                                             0  \n",
      "\n",
      "[50000 rows x 100935 columns]\n"
     ]
    }
   ],
   "source": [
    "#Creating the actual model and creating the features for the text data\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "cv = CountVectorizer(stop_words = 'english',ngram_range = (1,1),tokenizer = token.tokenize)\n",
    "text_counts = cv.fit_transform(df['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a8139ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 74772)\t1\n",
      "  (0, 56931)\t1\n",
      "  (0, 97369)\t2\n",
      "  (0, 47925)\t2\n",
      "  (0, 58)\t1\n",
      "  (0, 64779)\t6\n",
      "  (0, 29727)\t2\n",
      "  (0, 52630)\t3\n",
      "  (0, 42348)\t1\n",
      "  (0, 75187)\t2\n",
      "  (0, 30582)\t1\n",
      "  (0, 39959)\t1\n",
      "  (0, 11794)\t6\n",
      "  (0, 89582)\t1\n",
      "  (0, 85827)\t2\n",
      "  (0, 12724)\t1\n",
      "  (0, 93782)\t1\n",
      "  (0, 78020)\t1\n",
      "  (0, 96301)\t4\n",
      "  (0, 79573)\t1\n",
      "  (0, 99207)\t2\n",
      "  (0, 92127)\t1\n",
      "  (0, 31520)\t1\n",
      "  (0, 40679)\t1\n",
      "  (0, 90152)\t1\n",
      "  :\t:\n",
      "  (49999, 93820)\t1\n",
      "  (49999, 51434)\t1\n",
      "  (49999, 45528)\t1\n",
      "  (49999, 30926)\t1\n",
      "  (49999, 6952)\t1\n",
      "  (49999, 40045)\t1\n",
      "  (49999, 61053)\t1\n",
      "  (49999, 44402)\t1\n",
      "  (49999, 44081)\t1\n",
      "  (49999, 31756)\t1\n",
      "  (49999, 5840)\t1\n",
      "  (49999, 91618)\t1\n",
      "  (49999, 49264)\t1\n",
      "  (49999, 84023)\t1\n",
      "  (49999, 56084)\t1\n",
      "  (49999, 73975)\t1\n",
      "  (49999, 29728)\t1\n",
      "  (49999, 13598)\t1\n",
      "  (49999, 15684)\t1\n",
      "  (49999, 30938)\t1\n",
      "  (49999, 37696)\t1\n",
      "  (49999, 73968)\t1\n",
      "  (49999, 59753)\t1\n",
      "  (49999, 20970)\t1\n",
      "  (49999, 100182)\t1\n"
     ]
    }
   ],
   "source": [
    "print(text_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3267d46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(text_counts, df['sentiment'], test_size = 0.25,random_state = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b9fb09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Multinomial Naive Bayes Classifier for text analysis\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e08eadce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MNB = MultinomialNB()\n",
    "MNB.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df9ff067",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting the score\n",
    "from sklearn import metrics\n",
    "predicted = MNB.predict(X_test)\n",
    "accuracy_score = metrics.accuracy_score(predicted, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "342d992f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.85936\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4e0770db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.88088\n"
     ]
    }
   ],
   "source": [
    "#Accuracy of the unigram model is over 85%, we can try with bigram and trigram\n",
    "\n",
    "#Creating the actual model and creating the features for the text data\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "cv = CountVectorizer(stop_words = 'english',ngram_range = (1,2),tokenizer = token.tokenize)\n",
    "text_counts = cv.fit_transform(df['review'])\n",
    "\n",
    "#Splitting into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(text_counts, df['sentiment'], test_size = 0.25,random_state = 5)\n",
    "\n",
    "#Using Multinomial Naive Bayes Classifier for text analysis\n",
    "MNB_2 = MultinomialNB()\n",
    "MNB_2.fit(X_train,Y_train)\n",
    "\n",
    "#Predicting for new data and calculating accuracy\n",
    "predicted = MNB_2.predict(X_test)\n",
    "accuracy_score = metrics.accuracy_score(predicted, Y_test)\n",
    "print(accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "289d91be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.88216\n"
     ]
    }
   ],
   "source": [
    "#NGRAM_RANGE(1,2) change increased the accuray to 88% from 85%, we can try with (1,3) now\n",
    "\n",
    "#Creating the actual model and creating the features for the text data\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "cv = CountVectorizer(stop_words = 'english',ngram_range = (1,3),tokenizer = token.tokenize)\n",
    "text_counts = cv.fit_transform(df['review'])\n",
    "\n",
    "#Splitting into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(text_counts, df['sentiment'], test_size = 0.25,random_state = 5)\n",
    "\n",
    "#Using Multinomial Naive Bayes Classifier for text analysis\n",
    "MNB_3 = MultinomialNB()\n",
    "MNB_3.fit(X_train,Y_train)\n",
    "\n",
    "#Predicting for new data and calculating accuracy\n",
    "predicted = MNB_3.predict(X_test)\n",
    "accuracy_score = metrics.accuracy_score(predicted, Y_test)\n",
    "print(accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bf6d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXPLORING DIFFERENT TYPES OF NAIVE BAYES ALGORITHMS NOW \n",
    "#USED MULTINOMIAL NB TO GET 88%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c50b21f",
   "metadata": {},
   "source": [
    "Term Frequency-Inverse Document Frequency\n",
    "Let’s use TF-IDF, which takes in account the product of term frequency and inverse document frequency. Term frequency is how frequently a term has appeared in a document. Let’s say a term appears ‘f’ times in a document with ‘d’ words.\n",
    "Term Frequency = f/d\n",
    "IDF is ‘Inverse Document Frequency’. If a corpus contains N documents and the term of our interest appears only in D documents then IDF is:\n",
    "IDF = log(N/D) TF-IDF is product of Term Frequency and Inverse Document Frequency. TF-IDF shows the rarity of a word in the corpus. If a word is rare then probably its a signature word for a particular sentiment/information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3423476e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.88456\n"
     ]
    }
   ],
   "source": [
    "#Used TFIDF as the text prepocessing step and increased the accuracy by 0.2%\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "tfidf = TfidfVectorizer(stop_words = 'english',ngram_range = (1,3),tokenizer = token.tokenize)\n",
    "text_freq = tfidf.fit_transform(df['review'])\n",
    "\n",
    "#Splitting into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(text_freq, df['sentiment'], test_size = 0.25,random_state = 5)\n",
    "\n",
    "#Using Multinomial Naive Bayes Classifier for text analysis\n",
    "MNB_tfidf = MultinomialNB()\n",
    "MNB_tfidf.fit(X_train,Y_train)\n",
    "\n",
    "#Predicting for new data and calculating accuracy\n",
    "predicted = MNB_tfidf.predict(X_test)\n",
    "accuracy_score = metrics.accuracy_score(predicted, Y_test)\n",
    "print(accuracy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6baf1d60",
   "metadata": {},
   "source": [
    "Trying non-Bayesian algorithms\n",
    "Even the Tfidf vectorizer i.e creating a different BOW didn’t help in improving the accuracy of the model. Rather than naive Bayes algorithm we can also opt for stochastic gradient descent classifier or linear support vector classifier. Both of these are known to work well with the text data classification. Let’s try to use these:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f87956c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.88304\n"
     ]
    }
   ],
   "source": [
    "#implemented logistic regression and achieved similar accuracy\n",
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "tfidf = TfidfVectorizer(stop_words = 'english',ngram_range = (1,3),tokenizer = token.tokenize)\n",
    "text_freq = tfidf.fit_transform(df['review'])\n",
    "\n",
    "#Splitting into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(text_freq, df['sentiment'], test_size = 0.25,random_state = 5)\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logisticmodel = LogisticRegression()\n",
    "logisticmodel.fit(X_train,Y_train)\n",
    "\n",
    "#Predicting for new data and calculating accuracy\n",
    "predicted = logisticmodel.predict(X_test)\n",
    "accuracy_score = metrics.accuracy_score(predicted, Y_test)\n",
    "print(accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d80fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#implemented Support Vector Classifier and SGDClassifier\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
